Title (refer to HAR)
==============================
```{r, echo=FALSE, results='hide'}
library(caret)
library(ggplot2)
library(lattice)
library(klaR)
library(MASS)

```
You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did.

## Summary

## Model Construction
see scratch notes
set our global seed; this will be relevant if the tangled version of the Rmd is run.  alternatively can set individual seeds for every action that would require them:
```{r}
set.seed(2138)
```

### Obtaining the data
<R code chunk with links, download to data dir; can hide it but refer to it>
```{r, echo=FALSE, eval=FALSE}
#download training/testing datasets and load into R
trainingDataURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testDataURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
if(!file.exists("./data")) {dir.create("./data")}
download.file(trainingDataURL, destfile="./data/WLtrainingData.csv")
download.file(testDataURL, destfile="./data/WLtestData.csv")
dateDownloaded <- date()
training <- read.csv("./data/WLtrainingData.csv")
test <- read.csv("./data/WLtestData.csv")
```

### Exploration and Pre-processing to create model

<think carefully about this; push a few exploratory figures to an appendix if there are any>

NEED TO DEAL WITH THE FACTOR VARIABLES HERE - turn into dummies or whatever, but will cause model fits to crash if they're in there

## REMOVE ALL NON-SENSOR DATA VARIABLES FROM SET - define which columns to toss, then toss from both training and testing

use something like:

keepNames <- grep(c(pattern 1, pattern 2, pattern 3, etc. - should only be a dozen or so patterns the way that the variables are named))

then subset training and test on columns where names %in% keepNames

a main thing to do here is probably PCA - high dimensionality of the data may boil down to a small number of PCs.  If analysis doesn't yield anything interesting, can explore alternatives

can also look at some other factors, like structure of the data - lots of factor variables here - what does that imply for our model choice?  maybe something nonlinear would be better.  lots of missing values also.

regularization?  look at lecture on preprocessing and explore options for building some preprocessing into the model

### Cross Validation in our model

how to do this ?  manually, somehow?  or just note that it's being CV'd in the train (or trainControl ?) function?  look through lecture notes

can specify the method in trainControl(method = "cv") etc. - basically i think just note that this was passed (train() will bootstrap by default, but we can use a different method), and write a justification for why we chose what we did

### Explanation of Choices Made 

(maybe part of model construction; subsection - talk about exploratory stuff here and possibly touch upon overfitting)

## The final model - interpretation (?) if possible



## Expectations for Out-of-Sample Error

that the error rate will be higher than what we got on the test set -- more detail than this ?  see lecture notes on out-of-sample error

address overfitting here - what we have done to try to correct for it

## anything else that may be missing from assignment requirements

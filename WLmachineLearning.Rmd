Title (refer to HAR)
==============================
```{r, echo=FALSE, results='hide'}
library(caret)
library(ggplot2)
library(lattice)
library(klaR)
library(MASS)

```
You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did.

## Summary

## Model Construction
set our global seed; this will be relevant if the tangled version of the Rmd is run.  alternatively can set individual seeds for every action that would require them:
```{r}
set.seed(2138)
```

### Obtaining the data
<R code chunk with links, download to data dir; can hide it but refer to it>
```{r, echo=FALSE, eval=FALSE}
#download training/testing datasets and load into R
trainingDataURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testDataURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
if(!file.exists("./data")) {dir.create("./data")}
download.file(trainingDataURL, destfile="./data/WLtrainingData.csv")
download.file(testDataURL, destfile="./data/WLtestData.csv")
dateDownloaded <- date()
```
```{r}
training <- read.csv("./data/WLtrainingData.csv")
test <- read.csv("./data/WLtestData.csv")
```

### Exploration and Pre-processing

Some examination of the dataset reveals a number of features that will be superfluous to our model; our model will attempt to use sensor data to predict a human activity being performed, so first we will do some pre-processing to clean the data set to remove features that provide no information and/or do not relate to sensor inputs.

First we eliminate features for which there is almost no data available, as these will not inform our model:
```{r}
# get the columns that are mostly empty
colpctsNA <- rep(NA, ncol(training))
for(i in 1:ncol(training)){
        colpctsNA[i] <- sum(is.na(training[,i]))/nrow(training)
}
```
We can see that for 67 features, about 98% of the values are missing, so we discard these:
```{r}
table(colpctsNA)
# keep only the features that have substantial data in them
training <- training[,colpctsNA==0]
test <- test[,colpctsNA==0]
```

Next, we will strip out factor columns and features that show time or index type variables.  Recall that we would like to build our model based on the sensor inputs, so factors that represent a tangential aspect of the sensor measurement (kurtosis, skewness) will be discarded:
```{r}
# strip out the factor columns and time/index columns, keeping just the sensor
# columns and the target variable
factorcols <- sapply(training[1,], is.factor)
nonfactorcols <- factorcols[factorcols==FALSE]
keep <- c(names(nonfactorcols)[5:length(names(nonfactorcols))], "classe")
# keep the non-factor columns minus time and index, plus the target variable
training <- training[,keep]
#test <- test[,keep] need to figure out how to deal with this - looks like for problem submission
```

Our data are now processed, and ready to be used for model training.

## The final model - Construction, Cross-Validation, and Interpretation

For our data, which include 5 categorical possible outcomes, we will construct a random forest model -- the advantages of this model type in our situation are that it does not rely on linearity of the data, and does not assume anything about independence of the features, neither of which would likely be valid assumptions in sensor data coming from similar human activities under instruction to perform them slightly differently.

In our training function, we use only a randomly-sampled subset of 3500 out of the roughly 19,600 observations available to us to cut down on processing time.  We also specify our cross-validation in the training function, using 10 folds.  The code to build the model and the model summary itself are below:

```{r}
set.seed(256)
trainingSub <- training[sample(nrow(training), 3500), ]
fitRF <- train(classe~., data=trainingSub, method="rf", trControl=trainControl(method="cv", number=10))
fitRF
```



### Explanation of Choices Made 

(maybe part of model construction; subsection - talk about exploratory stuff here and possibly touch upon overfitting)

### Expectations for Out-of-Sample Error

that the error rate will be higher than what we got on the test set -- more detail than this ?  see lecture notes on out-of-sample error

address overfitting here - what we have done to try to correct for it

## anything else that may be missing from assignment requirements
